---
title: "Regularized VSUPs"
output: 
  html_document:
    toc: true
    toc_float: true
    fig_caption: false
    self_contained: true
---

_Matthew Kay <mjskay@umich.edu>_

## Introduction

I was reading through the [VSUP paper](https://idl.cs.washington.edu/files/2018-UncertaintyPalettes-CHI.pdf) and it occurred to me that there is a connection between this and the idea of shrinkage in hierarchical models --- specifically, for groups with higher uncertainty in hierarchical models, estimates will be pulled more towards the mean, making them more conservative (or, say, information from more certain groups is borrowed to improve estimates in groups that are less certain). Another way of viewing this is as a form of regularization.

At first I thought that that is what VSUPs would do, but then I realized they do so inconsistently --- sometimes they pull values towards the mean, and sometimes they actually pull values away from the mean. I tried to illustrate:

![tree versus shrinkage vsup](tree_vs_shrinkage_vsup.jpg)

The tree-based VSUP leads to situations where the color value for some cells move towards the mean at some levels of uncertainty and away from the mean at other levels of uncertainty, which I think might explain some results like people being more likely to select certain strictly worse cells, circled here (the lower of the two circled cells has equal danger but higher uncertainty, so there is not really a situation I can think of where one would want it to be more likely to be selected):

![order reversal](order_reversal.png)

I really like the general idea of making estimates more conservative under higher uncertainty by manipulating the encoding. I wondered if connecting this to a shrinkage estimator might yield a similar sort of palette but where the color is always pulled towards the mean, making reversals like the one above less likely. The thing that might be lost is the additional fidelity at high-certainty ends of the scale.

There's a Bayesian framing to the regularization approach as well which Michael and I discussed a bit already: that at higher uncertainty, the prior dominates, and when uncertainty is low, the data dominates. The nice thing is that all these things turn out to be different sides of the same coin (regularization/shrinkage, hierarchical models, and a Bayesian view where the shrinkage is determined by a prior). 

In the rest of this document I'm going to try to actually build some VSUP-like-things using regularization / a Bayesian perspective.

## Setup

```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(magrittr)
library(rstan)
library(brms)
library(tidybayes)
library(ggstance)
library(modelr)
library(patchwork)
library(LaplacesDemon)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
theme_set(theme_light())
```

### Load data

Load the polling data...

```{r}
polls = read_csv("examples/polling.csv",
  col_types = cols(
    State = col_character(),
    Date = col_character(),
    `Hillary Clinton` = col_number(),
    `Donald Trump` = col_number(),
    `Gary Johnson` = col_number(),
    `Jill Stein` = col_number(),
    `Evan McMullin` = col_number(),
    `Margin of error` = col_number(),
    Lead = col_number(),
    `Clinton potential EVs` = col_number(),
    `Trump potential EVs` = col_number(),
    `Tied potential EVs` = col_number(),
    Result = col_double()
  ),
  na = c("", "Tied")
) %>%
  mutate(
    #need to recalculate this as it is not signed in the data
    lead = `Hillary Clinton` - `Donald Trump`,
    se = `Margin of error`
  )
```

## Regularizing the estimates

We'll make a few assumptions:

1. `Margin of error` in the table is the standard error
2. Everything is Gaussian so we can use a Gaussian conjugate prior to regularize

We'll use an empirical 0-centered prior by taking the variance of all the leads *as if the mean lead were actually 0* (which it isn't, it is slightly less than 0) as the variance of our prior. That is:

```{r}
prior_var = mean(polls$lead ^ 2)
```

Thus our prior is _N_(0, `prior_var`) = _N_(0, `r prior_var`). I chose 0-centered here (rather than centered on the actual mean, which is `r mean(polls$lead)`, because of some half-remembered argument from Michael that 0-centered is more neutral in polling data. You could equally as well do everything that follows from here by shrinking towards sample mean instead of assuming a mean of 0. The variance would change a bit, as would the calculation of the adjusted mean.

We will regularize by adjusting each mean `lead` to be the posterior mean from a Gaussian model if we had used _N_(0, `r prior_var`) as the prior on the mean. That is, if the sample mean is $\bar{x}$ (here, `lead`), the margin of error is $\sigma_\bar{x}$ (here, `se`), and the variance of the prior is $\sigma_0^2$ (here `prior_var`), then the posterior mean $\mu_n$ is (based on [this](https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf) plus some re-arranging and substitution):

$$
\mu_n = \frac{\bar{x}}{1 + \frac{\sigma_\bar{x}^2}{\sigma_0^2}}
$$

Thus:

```{r}
polls %<>% mutate(
  lead_adj = lead / (1 + se^2 / prior_var)
)
```

Let's see what the adjustment (aka shrinkage aka regularization) looks like:

```{r, fig.width = 10, fig.height = 5}
set.seed(123456)
polls %>%
  mutate(y = -runif(n())/sqrt(prior_var)/5) %>%
  ggplot(aes(y = y, yend = y, xend = lead, x = lead_adj)) +
  stat_function(
    fun = function(x) dnorm(x, 0, sqrt(prior_var)),
    geom = "area", xlim = c(-50, 50), fill = "gray85"
  ) +
  geom_segment(size = 0.75, color = "red") +
  geom_point(size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(
    "text", x = 37, y = 0.004, label = "prior", color = "gray50",
    hjust = 0, fontface = "bold"
  ) +
  annotate(
    "text", x = 37, y = -0.0088, label = "shrinkage", color = "red", 
    hjust = 0, fontface = "bold"
  ) +
  scale_y_continuous(breaks = NULL) +
  ylab(NULL)
```

Black dots indicate the adjusted lead; red lines show shrinkage/regularization from the unadjusted values. Values with greater shrinkage will be a result of some combination of being further from 0 and having more uncertainty.

To get a sense of how error and distance from 0 interact to produce shrinkage, let's try to see what that looks like over the space of values in the data (__Note:__ in this plot, `se` increases as you go downwards for consistency with the way the color palettes are drawn):

```{r}
polls %>%
  data_grid(
    lead = max(abs(lead)) %>% seq(-., ., length.out = 19),
    se = seq_range(se, n = 20)
  ) %>%
  mutate(lead_adj = lead / (1 + se^2 / prior_var)) %>%
  ggplot(aes(y = se, yend = se, xend = lead, x = lead_adj)) +
  geom_segment(size = 0.75, color = "red") +
  geom_line(aes(group = lead), color = "gray75") +
  geom_point(size = 1.5) +
  scale_y_reverse() +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  xlab("Adjusted lead (connected dots have the same unadjusted lead)")
```

## Regularized VSUPs

### On the original data

Okay, let's try to define some small number of cells based on this to make a color scale...

```{r}
shrinkage_adjustment = function(lead, se, prior_var) {
  lead / (1 + se^2 / prior_var)
}

build_scale = function(min_lead, max_lead, min_se, max_se, adjust, ..., uncertainty_var = se) {
  uv = ensym(uncertainty_var)
  
  crossing(
    lead = seq(min_lead, max_lead, length.out = 5),
    !!uv := seq(min_se, max_se, length.out = 5)
  ) %>%
  group_by(lead) %>%
  mutate(
    prev_se = lag(!!uv),
    se_mid = (prev_se + !!uv)/2
  ) %>%
  group_by(!!uv) %>%
  mutate(
    prev_lead = lag(lead), 
    lead_mid = (lead + prev_lead)/2,
    lead_adj = adjust(lead, se_mid, ...),
    prev_lead_adj = lag(lead_adj),
    lead_mid_adj = adjust(lead_mid, se_mid, ...)
  ) %>%
  drop_na()
}

scale_data = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), max(se), shrinkage_adjustment, prior_var = prior_var)
```

Then compare the two versions. First, we'll color each cell by the value of `lead` at its midpoint (left plot) then by the adjusted (shrunken / regularized) value at its midpoint (right plot):

```{r fig.width = 6, fig.height = 3}
draw_scale = function(scale_data, color_by = "lead_mid", uncertainty_var = se) {
  uv = ensym(uncertainty_var)

  scale_data %>%
    mutate(lead_mid = !!sym(color_by)) %>%
    ggplot(aes(
      xmin = prev_lead, xmax = lead, 
      ymin = prev_se, ymax = !!uv, alpha = -se_mid, fill = lead_mid
    )) + 
    geom_rect() +
    scale_y_reverse() +
    scale_fill_gradient(
      # limits = max(abs(scale_data$lead)) %>% c(-., .),
      low = rgb(166, 28, 58, max = 255), high = rgb(57, 93, 159, max = 255),
      guide = FALSE
    ) +
    scale_x_continuous(position = "top", limits = c(-40, 40)) +
    scale_alpha_continuous(guide = FALSE, range = c(0.25,1)) +
    theme(panel.grid = element_blank()) +
    ylab(paste0("uncertainty (", as.character(uv), ")")) +
    xlab("lead")
}

no_shrink = scale_data %>%
  draw_scale(color_by = "lead_mid") +
  ggtitle("No shrinkage")

shrink = scale_data %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("Shrinkage")

no_shrink | shrink
```

There are differences in color, but they are very minor. The problem is that this particular dataset doesn't have data points with huge uncertainty, so there isn't much shrinkage needed to adjust for it.

### With more error

To see what happens if there are data points with a lot of shrinkage, let's let the maximum standard error be much larger:

```{r fig.width = 6, fig.height = 3}
no_shrink_large = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale(color_by = "lead_mid") +
  ggtitle("No shrinkage, large max SE")

shrink_large = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("Shrinkage, large max SE")

no_shrink_large | shrink_large
```

Now you can see the difference! And that last category is almost blurring itself into one.

### And a distorted layout

Or you could play with the layout a bit (this would need work):

```{r fig.width = 6, fig.height = 3}
draw_scale_distorted = function(scale_data) {
  scale_data %>%
    ggplot(aes(
      xmin = prev_lead_adj, xmax = lead_adj, 
      ymin = prev_se, ymax = se, alpha = -se_mid, fill = lead_mid_adj
    )) + 
    geom_rect() +
    scale_y_reverse() +
    scale_fill_gradient(
      # limits = max(abs(scale_data$lead)) %>% c(-., .),
      low = rgb(166, 28, 58, max = 255), high = rgb(57, 93, 159, max = 255),
      guide = FALSE
    ) +
    scale_alpha_continuous(guide = FALSE, range = c(0.25,1)) +
    scale_x_continuous(position = "top", limits = c(-40, 40)) +
    theme(panel.grid = element_blank()) +
    ylab("se") +
    xlab("lead")
}

distorted_small = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), max(se), shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale_distorted() +
  ggtitle("Shrinkage, small SE")

distorted_large = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale_distorted() +
  ggtitle("Shrinkage, large SE")

distorted_small | distorted_large

```

This would probably be improved more if the rectangles were polygons so that the boundaries between leads lined up across rows, but I think the idea gets across.


## Thoughts

I think that some variant of regularization could offer a way to justify color choice in something like a VSUP: it can answer the question, _how much_ color value supressing should the palette do? It also has some nice properties, like avoiding order reversals that happen in tree-based VSUPs.

On the other hand, it does not immediately seem to have the property of allowing fewer colors to be used, which is a nice property of tree-based VSUPs. Maybe there is a way to do that? Perhaps one could come up with a way of merging categories at lower levels based on perceptual distance of the colors or something. However, I think this would be very dataset-dependent: the polling data above does not actually warrant much shrinkage or value suppressing, in my opinion, so drastic value suppressing or merging of categories seems like an overcorrection---unless you had a very strong prior saying that polling numbers should be close to 50-50.

## Addendum: making the boundaries line up

```{r, fig.width = 6, fig.height = 3}
build_scale_polygon = function(min_lead, max_lead, min_se, max_se, 
  adjust, ..., se_breaks = 4, lead_breaks = 4
) {
  crossing(
    lead = seq(min_lead, max_lead, length.out = lead_breaks + 1),
    se = seq(min_se, max_se, length.out = se_breaks + 1)
  ) %>%
  group_by(lead) %>%
  mutate(
    se_bottom = se,
    se_top = lag(se_bottom),
    se_mid = (se_top + se_bottom)/2
  ) %>%
  group_by(se) %>%
  mutate(
    lead_right = lead,
    lead_left = lag(lead_right), 
    lead_mid = (lead_right + lead_left)/2,
    lead_right_adj = adjust(lead_right, se_mid, ...),
    lead_left_adj = lag(lead_right_adj),
    lead_mid_adj = adjust(lead_mid, se_mid, ...)
  ) %>%
  group_by(lead) %>%
  mutate(
    lead_left_top_adj = lag(lead_left_adj) %>% ifelse(is.na(.), lead_left, .),
    lead_right_top_adj = lag(lead_right_adj) %>% ifelse(is.na(.), lead_right, .)
  ) %>%
  drop_na() %>%
  group_by(lead_mid_adj, se_mid) %>%
  do(with(., data_frame(
    lead = c(lead_left_top_adj, lead_left_adj, lead_right_adj, lead_right_top_adj),
    se = c(se_top, se_bottom, se_bottom, se_top)
  )))
}

draw_scale_polygon = function(scale_data){
  scale_data %>%
    ggplot(aes(
      x = lead, y = se, fill = lead_mid_adj, 
      group = paste(lead_mid_adj,se_mid), alpha = -se_mid
    )) +
    geom_polygon() +
    scale_y_reverse() +
    scale_fill_gradient(
      low = rgb(166, 28, 58, max = 255), high = rgb(57, 93, 159, max = 255),
      guide = FALSE
    ) +
    scale_alpha_continuous(guide = FALSE, range = c(0.25,1)) +
    scale_x_continuous(position = "top") +
    theme(panel.grid = element_blank()) +
    ylab("se") +
    xlab("lead")
}

polygon_small = polls %$%
  build_scale_polygon(-max(abs(lead)), max(abs(lead)), min(se), max(se), shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale_polygon() +
  ggtitle("Shrinkage, small SE")

polygon_large = polls %$%
  build_scale_polygon(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale_polygon() +
  ggtitle("Shrinkage, large SE")

polygon_small | polygon_large
```

Or with more breaks:

```{r, fig.width = 6, fig.height = 3}
polygon_small = polls %$%
  build_scale_polygon(-max(abs(lead)), max(abs(lead)), min(se), max(se), shrinkage_adjustment, prior_var = prior_var,
    lead_breaks = 5, se_breaks = 5) %>%
  draw_scale_polygon() +
  ggtitle("Shrinkage, small SE")

polygon_large = polls %$%
  build_scale_polygon(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var,
    lead_breaks = 5, se_breaks = 5) %>%
  draw_scale_polygon() +
  ggtitle("Shrinkage, large SE")

polygon_small | polygon_large
```

I think this starts to have a more VSUP-y flavor. For example, the cells in bottom row or two in the large-SE case start to blur together --- so you can get a merging of colors, though it is as a consequence of shrinkage + the colors being so close perceptually, not explicitly setting the colors to be the same.

## Addendum 2: polar coordinates (VSUP-style)

Turns out generating an approximation of the VSUP style just involves taking the square (non-distorted) version and then translating it into polar coordinates. Like so (the `|` and `&` operators here are from the `patchwork` package --- `|` lays out the plots beside each other and `&` applies ggplot functions to each subchart (like `+` does for a single chart in ggplot)):

```{r fig.width = 12, fig.height = 6}
(no_shrink_large | shrink_large) &
  coord_polar(start = pi) &
  # The constant 5 in the next line is how many times you would have to repeat the
  # chart to make a circle. Increase to make the chart narrower, decrease to make wider.
  {max(abs(polls$lead)*5) %>% xlim(-., .)} 
```

On the one hand, the shrinkage one is kind of nice because the shape reflects how much shrinkage there is. On the other hand, the polar coordinates version is kind of elegant, and may possibly be easier to annotate and explain.

Maybe a compromise would be to set the starting radius to something other than 0 and the angular extent of the plot so that the slope of the outer edges and the width of the most uncertain row is approximately the same as in the shrinkage chart. I'm sure you could figure out how to determine those values automatically, but I'm eye-balled it here for the large SE case:

```{r fig.width = 8, fig.height = 8}
shrink_large +
  coord_polar(start = pi) +
  #increase constant 8 to make chart narrower
  {max(abs(polls$lead)*8) %>% xlim(-., .)} +
  #increase constant 45 to increase radius on bottom row
  scale_y_reverse(limits = c(45, NA))
```

(compare the above chart to this one:)

```{r fig.width = 3, fig.height = 3}
polls %$%
  build_scale_polygon(-max(abs(lead)), max(abs(lead)), min(se), 30, shrinkage_adjustment, prior_var = prior_var) %>%
  draw_scale_polygon() +
  ggtitle("Shrinkage, large SE")
```

## Addendum 3: linear in log-odds model

### The linear in log-odds model

Let's imagine instead that we want to do a sort of perceptual correction: we expect that people's perception of the probability that Clinton will lose^[I've used $P(Clinton loses}$ instead of $P(Clinton wins)$ because it avoids a bunch of $1 - p$ in the math] is biased according to [linear in log-odds](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261445/) model, as many probabilities are.

Say the relationship between the perceived `P(Clinton loses)` and the reported `P(Clinton loses)` is linear in log-odds space, with slope `a` and intercept `b`. In the linear in log-odds model the particular values of `a` and `b` are domain-dependent. For the sake of argument, we'll choose `a` and `b` such that _perceived probability_ == _actual probability_ when _actual probability_ == 0.5, and so that people's perceived probabilities are generally biased _away_ from 0.5 (this is what folks often argue in the polling case: if I say someone has a 30% chance of losing (or a 70% chance of winning), people will think the candidate is more likely to lose (win) than they actually are). Thus:

$$
\textrm{logit}(p_{perceived}) = a + b\cdot\textrm{logit}(p_{actual})
$$

```{r, fig.width = 6, fig.height = 5}
a = 0
b = 1.5

data_frame(
  p_actual = seq(.001, .999, length.out = 101),
  p_perceived = invlogit(a + b*logit(p_actual))
) %>%
  ggplot(aes(x = p_actual, y = p_perceived)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_line(color = "red", size = 1) 
```

### Correction using the linear in log-odds model

Thus, to correct for misperceptions in uncertainty in the probability of losing that result from how people perceive the uncertainty in an estimate of the margin of victory (the `lead`), we need to (1) translate the `lead` and `se` into a lose probability we want people to perceive (`p_perceived`); (2) translate that perceived lose probability into a displayed lose probability (`p_displayed`) that, if shown, people _would perceive_ as `p_perceived`; (3) determine the value of lead that corresponds to that actual lose probability.

(1) To translate `lead` and `se` into a lose probability we want the audience to perceive (`p_perceived`), we'll use the CDF of the normal distribution:

$$
\begin{eqnarray}
p_{perceived}& = &P(\textrm{Clinton loses})\\
&=& P(lead < 0)\\
&=& F_{Normal}(0|lead, se)\\ 
\end{eqnarray}
$$
Where $F_{Normal}(x|\mu, \sigma)$ is the CDF of a normal distribution with mean $\mu$ and standard deviation $\sigma$. In R, this is the `pnorm` function:

```{r}
polls %<>% mutate(
  p_perceived = pnorm(0, lead, se)
)
```

(2) Now we'll translate $p_{perceived}$ into $p_{displayed}$ by inverting the linear in log-odds model:

$$
\begin{align}
&& \textrm{logit}(p_{perceived}) &= a + b\cdot\textrm{logit}(p_{displayed})\\
\implies && p_{displayed} &= \textrm{logit}^{-1}\Big(\frac{\textrm{logit}(p_{perceived}) - a}{b}\Big)
\end{align}
$$

```{r}
polls %<>% mutate(
  p_displayed = invlogit((logit(p_perceived) - a) / b)
)
```

(3) Finally, we'll calculate the lead necessary to acheive the lose probability we want to display, given the uncertainty in the estimate. We want $lead_{adj}$ such that:

$$
\begin{align}
p_{displayed}& = F_{Normal}(0|lead_{adj}, se)
\end{align}
$$

To determine $lead_{adj}$, we can use the fact that with the Normal distribution, $F_{Normal}(0|\mu, \sigma) = F_{Normal}(-mu|0, \sigma)$---that is, if we want to find the probability of getting a value less than 0 for a normally-distributed variable $X$ that is not centered on 0, this is the same as shifting the distribution to be centered on 0 and asking what the probability is of a value less than $-\mu$.

$$
\begin{align}
&& p_{displayed}& = F_{Normal}(0|lead_{adj}, se) \\
\implies && p_{displayed}& = F_{Normal}(-lead_{adj}|0, se) \\
\implies && F_{Normal}^{-1}(p_{displayed}|0,se)& = -lead_{adj} \\
\implies && lead_{adj}& = -F_{Normal}^{-1}(p_{displayed}|0,se)
\end{align}
$$

$F_{Normal}^{-1}$ is the Normal inverse CDF, or the quantile function of the Normal distribution, which is the `qnorm` function:

```{r}
polls %<>% mutate(
  lead_adj_llo = -qnorm(p_displayed, 0, se)
)
```

### Results of the correction

This correction yields a greater adjustment then the shrinkage approach (*N.B.* I completely made up the LLO slope parameter, `b`, which directly controls the size of the adjustment here):


```{r}
polls %>%
  ggplot(aes(x = se, y = abs(lead_adj_llo - lead))) +
  geom_point()
```

```{r, fig.width = 10, fig.height = 2}
set.seed(123456)
polls %>%
  mutate(y = -runif(n())) %>%
  ggplot(aes(y = y, yend = y, xend = lead, x = lead_adj_llo)) +
  geom_segment(size = 0.75, color = "red") +
  geom_point(size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(
    "text", x = 31, y = -.14, label = "adjustment", color = "red", 
    hjust = 0, fontface = "bold"
  ) +
  scale_y_continuous(breaks = NULL) +
  ylab(NULL)
```


### Palettes

To draw the palettes, we'll define an adjustment function that implements the above steps:

```{r}
llo_adjustment = function(lead, se, factor = 1) {
  # This function is a more numerically stable implementation of:
  #   p_perceived = pnorm(0, lead, se)
  #   p_displayed = invlogit((logit(p_perceived) - a) / b)
  #   -qnorm(p_displayed, 0, se)
  logit_p_perceived = pnorm(0, lead, se, log.p = TRUE) - pnorm(0, lead, se, log.p = TRUE, lower.tail = FALSE)
  logit_p_displayed = (logit_p_perceived - a) / b
  lead_adj = ifelse(logit_p_displayed > 0,
    qnorm(invlogit(-logit_p_displayed), 0, se),
    -qnorm(invlogit(logit_p_displayed), 0, se)
  )
  lead_adj * factor
}
```

#### Square
```{r, fig.width = 8.5, fig.height = 3}
polls %<>% mutate(
  lead_adj_llo = llo_adjustment(lead, se)
)
llo_factor = polls %$% {max(abs(lead)) / max(abs(lead_adj_llo))}

scale_data_shrink = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), max(se), shrinkage_adjustment, prior_var = prior_var)

scale_data_llo = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), max(se), llo_adjustment, factor = llo_factor)

no_shrink = scale_data_shrink %>%
  draw_scale(color_by = "lead_mid") +
  ggtitle("No shrinkage")

shrink = scale_data_shrink %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("Shrinkage")

llo = scale_data_llo %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("LLO")

no_shrink | shrink | llo
```

#### Distorted (shrunken rows)

```{r, fig.width = 8.5, fig.height = 3}
shrink_distorted = scale_data_shrink %>%
  draw_scale_distorted() +
  ggtitle("Shrinkage")

llo_distorted = scale_data_llo %>%
  draw_scale_distorted() +
  ggtitle("LLO")

no_shrink | shrink_distorted | llo_distorted
```

#### With more error

```{r fig.width = 6, fig.height = 3}
llo_distorted_large = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(se), 30, llo_adjustment, factor = llo_factor) %>%
  draw_scale_distorted() +
  ggtitle("LLO, large max SE")

llo_distorted | llo_distorted_large
```

## Addendum 4: linear in log-odds model, uncertainty as P(loss)

One issue with the above linear-in-log-odds approach might be the use of `se` for the uncertainty: it might make more sense to quantify the uncertainty in this model as $P(Clinton~loses)$, or more precisely, $.5 - |P(Clinton~loses) - .5|$, i.e., the probability that whoever is not in the lead wins.

```{r}
polls %<>% mutate(
  p_underdog_wins = .5 - abs(p_perceived - .5)
)
```


```{r, fig.width = 10, fig.height = 2}
polls %>%
  ggplot(aes(y = p_underdog_wins, yend = p_underdog_wins, xend = lead, x = lead_adj_llo)) +
  geom_segment(size = 0.75, color = "red") +
  geom_point(size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(
    "text", x = 31, y = -.14, label = "adjustment", color = "red", 
    hjust = 0, fontface = "bold"
  ) +
  scale_y_reverse() +
  ylab(NULL)
```

## Palettes 

```{r}
llo_u_adjustment = function(lead, P_underdog_wins, factor = 1) {
  # This function is a more numerically stable implementation of:
  #   p_perceived = pnorm(0, lead, se)
  #   p_displayed = invlogit((logit(p_perceived) - a) / b)
  #   -qnorm(p_displayed, 0, se)
  logit_p_perceived = logit(P_underdog_wins)
  logit_p_perceived = pnorm(0, lead, se, log.p = TRUE) - pnorm(0, lead, se, log.p = TRUE, lower.tail = FALSE)
  logit_p_displayed = (logit_p_perceived - a) / b
  lead_adj = ifelse(logit_p_displayed > 0,
    qnorm(invlogit(-logit_p_displayed), 0, se),
    -qnorm(invlogit(logit_p_displayed), 0, se)
  )
  lead_adj * factor
}

```


```{r}
scale_data_llo_u = polls %$%
  build_scale(-max(abs(lead)), max(abs(lead)), min(p_underdog_wins), max(p_underdog_wins), llo_adjustment, factor = llo_factor, uncertainty_var = p_underdog_wins)

no_shrink = scale_data_shrink %>%
  draw_scale(color_by = "lead_mid") +
  ggtitle("No shrinkage")

shrink = scale_data_shrink %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("Shrinkage")

llo = scale_data_llo %>%
  draw_scale(color_by = "lead_mid_adj") +
  ggtitle("LLO")

llo_u = scale_data_llo_u %>%
  draw_scale(color_by = "lead_mid_adj", uncertainty_var = p_underdog_wins) +
  ggtitle("LLO (U)")

llo_u

no_shrink | shrink | llo

```


## Appendix: verifying the math (for regularized version)

Just to make sure I didn't screw up some aspect of the math in the analytical version of the model above, here is basically the same model fit using MCMC:

```{r}
m = brm(lead | se(se) ~ 0 + (1|State), data = polls)
```

The results are identical:

```{r}
polls %>%
  add_fitted_samples(m) %>%
  mean_qi() %>%
  ggplot(aes(x = lead_adj, y = estimate)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  xlab("Adjusted lead from analytical method (Empirical Bayes)") +
  ylab("Adjusted lead from MCMC")
```

